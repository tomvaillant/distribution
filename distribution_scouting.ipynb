{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3356e4b0-9c0e-48c6-84d2-4f1761b115b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "# Load competitor sources from CSV file\n",
    "def load_competitor_sources_from_csv(csv_path='competitor_sources.csv'):\n",
    "    \"\"\"Load competitor sources from CSV file and organize by continent.\"\"\"\n",
    "    try:\n",
    "        sources_df = pd.read_csv(csv_path)\n",
    "        print(f\"\u2705 Loaded {len(sources_df)} competitor sources from {csv_path}\")\n",
    "        \n",
    "        # Organize by continent\n",
    "        news_feeds = {}\n",
    "        for continent in sources_df['continent'].unique():\n",
    "            continent_sources = sources_df[sources_df['continent'] == continent]\n",
    "            news_feeds[continent] = []\n",
    "            \n",
    "            for _, row in continent_sources.iterrows():\n",
    "                news_feeds[continent].append({\n",
    "                    'name': row['name'],\n",
    "                    'url': row['url']\n",
    "                })\n",
    "        \n",
    "        return news_feeds\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\u274c CSV file '{csv_path}' not found!\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error loading CSV: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Function to extract time ago from span text\n",
    "def extract_time_ago(time_text):\n",
    "    \"\"\"Convert time ago text to minutes for sorting.\"\"\"\n",
    "    if not time_text:\n",
    "        return 999999  # Large number for unknown times\n",
    "    \n",
    "    time_text = time_text.lower()\n",
    "    if 'm' in time_text or 'min' in time_text:\n",
    "        match = re.search(r'(\\d+)', time_text)\n",
    "        return int(match.group(1)) if match else 999999\n",
    "    elif 'h' in time_text or 'hour' in time_text:\n",
    "        match = re.search(r'(\\d+)', time_text)\n",
    "        return int(match.group(1)) * 60 if match else 999999\n",
    "    elif 'd' in time_text or 'day' in time_text:\n",
    "        match = re.search(r'(\\d+)', time_text)\n",
    "        return int(match.group(1)) * 1440 if match else 999999\n",
    "    else:\n",
    "        return 999999\n",
    "\n",
    "# Function to scrape Bing News results\n",
    "def scrape_bing_news(url, source_name, max_articles=10):\n",
    "    \"\"\"Scrape news articles from Bing News search results.\"\"\"\n",
    "    articles = []\n",
    "    \n",
    "    try:\n",
    "        print(f\"  Scraping {source_name}...\", end=\"\", flush=True)\n",
    "        \n",
    "        # Headers to mimic a real browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "        \n",
    "        # Make request\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all news cards\n",
    "        news_cards = soup.find_all('div', class_='news-card')\n",
    "        \n",
    "        if not news_cards:\n",
    "            # Try alternative selectors\n",
    "            news_cards = soup.find_all('div', {'class': re.compile('newsitem|news-card-body')})\n",
    "        \n",
    "        print(f\"\\n    \ud83d\udccb Found {len(news_cards)} news items\")\n",
    "        \n",
    "        for i, card in enumerate(news_cards[:max_articles]):\n",
    "            try:\n",
    "                article_data = {}\n",
    "                \n",
    "                # Extract title\n",
    "                title_elem = card.find('a', class_='title')\n",
    "                if not title_elem:\n",
    "                    title_elem = card.find('a', {'class': re.compile('title|headline')})\n",
    "                \n",
    "                if title_elem:\n",
    "                    article_data['title'] = title_elem.get_text(strip=True)\n",
    "                    article_data['link'] = title_elem.get('href', '')\n",
    "                else:\n",
    "                    continue  # Skip if no title found\n",
    "                \n",
    "                # Extract snippet/description\n",
    "                snippet_elem = card.find('div', class_='snippet')\n",
    "                if not snippet_elem:\n",
    "                    snippet_elem = card.find('div', {'class': re.compile('snippet|summary|description')})\n",
    "                \n",
    "                article_data['description'] = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
    "                \n",
    "                # Extract time\n",
    "                time_elem = card.find('span', {'aria-label': re.compile('ago|minutes|hours|days')})\n",
    "                if not time_elem:\n",
    "                    time_elem = card.find('span', text=re.compile(r'\\d+[mhd]'))\n",
    "                \n",
    "                time_text = time_elem.get_text(strip=True) if time_elem else 'Unknown'\n",
    "                article_data['time_ago'] = time_text\n",
    "                article_data['minutes_ago'] = extract_time_ago(time_text)\n",
    "                \n",
    "                # Extract source (might be different from search source)\n",
    "                source_elem = card.find('a', {'aria-label': re.compile('Search news from')})\n",
    "                if source_elem:\n",
    "                    article_data['original_source'] = source_elem.get_text(strip=True)\n",
    "                else:\n",
    "                    article_data['original_source'] = source_name\n",
    "                \n",
    "                article_data['source'] = source_name\n",
    "                article_data['timestamp'] = datetime.now().isoformat()\n",
    "                \n",
    "                articles.append(article_data)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n    \u26a0\ufe0f  Error parsing article {i+1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Sort by recency\n",
    "        articles.sort(key=lambda x: x['minutes_ago'])\n",
    "        \n",
    "        print(f\"    \u2713 ({len(articles)} articles extracted)\")\n",
    "        \n",
    "        # Debug: Print first article\n",
    "        if articles:\n",
    "            first = articles[0]\n",
    "            print(f\"      First article: {first['title'][:60]}...\")\n",
    "            print(f\"      Time: {first['time_ago']}, Link: {first['link'][:50]}...\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\" \u2717 (Network error: {type(e).__name__})\")\n",
    "    except Exception as e:\n",
    "        print(f\" \u2717 (Error: {type(e).__name__}: {str(e)})\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Main execution\n",
    "print(\"Starting Bing News competitor scraper...\\n\")\n",
    "\n",
    "# Load competitor sources\n",
    "NEWS_FEEDS = load_competitor_sources_from_csv()\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Process each continent\n",
    "for continent, feeds in NEWS_FEEDS.items():\n",
    "    print(f\"\\n{continent}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for feed in feeds:\n",
    "        articles = scrape_bing_news(feed['url'], feed['name'])\n",
    "        \n",
    "        # Add continent info to each article\n",
    "        for article in articles:\n",
    "            article['continent'] = continent\n",
    "            article['feed_url'] = feed['url']\n",
    "            all_results.append(article)\n",
    "        \n",
    "        # Delay to be respectful and avoid rate limiting\n",
    "        time.sleep(2)\n",
    "\n",
    "# Create DataFrame\n",
    "print(f\"\\n\\nCreating DataFrame...\")\n",
    "news_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Show summary\n",
    "if not news_df.empty:\n",
    "    print(f\"\\n\u2705 Success! Collected {len(news_df)} articles from {news_df['source'].nunique()} sources\")\n",
    "    \n",
    "    print(f\"\\nArticles per source:\")\n",
    "    for source in news_df['source'].value_counts().index[:10]:\n",
    "        count = len(news_df[news_df['source'] == source])\n",
    "        print(f\"  {source}: {count}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udcf0 Sample headlines:\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, row in news_df.head(5).iterrows():\n",
    "        print(f\"{row['source']}: {row['title'][:80]}...\")\n",
    "        if row['description']:\n",
    "            print(f\"  \ud83d\udcc4 Description: {row['description'][:150]}...\")\n",
    "        print(f\"  \u23f0 Posted: {row['time_ago']}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Content statistics:\")\n",
    "    print(f\"  Articles with descriptions: {len(news_df[news_df['description'] != ''])}\")\n",
    "    print(f\"  Articles without descriptions: {len(news_df[news_df['description'] == ''])}\")\n",
    "    print(f\"  Average description length: {news_df['description'].str.len().mean():.0f} characters\")\n",
    "    \n",
    "    # Show recency distribution\n",
    "    print(f\"\\n\u23f1\ufe0f  Recency distribution:\")\n",
    "    recent_counts = news_df['time_ago'].value_counts().head(10)\n",
    "    for time_val, count in recent_counts.items():\n",
    "        print(f\"  {time_val}: {count} articles\")\n",
    "else:\n",
    "    print(\"\u274c No articles collected!\")\n",
    "\n",
    "# The DataFrame is now available as 'news_df'\n",
    "print(f\"\\n\\n\u2728 DataFrame stored in variable 'news_df' with {len(news_df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f76284-ced9-49c8-a914-68b92df8d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_20min_titles():\n",
    "    \"\"\"Fetch titles from 20min.ch API.\"\"\"\n",
    "    \n",
    "    url = \"https://api.20min.ch/content/6/category/1\"\n",
    "    \n",
    "    # Headers to simulate request from 20min.ch\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'de-CH,de;q=0.9,en;q=0.8',\n",
    "        'Origin': 'https://www.20min.ch',\n",
    "        'Referer': 'https://www.20min.ch/',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"Fetching data from 20min.ch API...\")\n",
    "        \n",
    "        # Make request\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse JSON\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract titles\n",
    "        titles = []\n",
    "        \n",
    "        if 'content' in data and 'elements' in data['content']:\n",
    "            elements = data['content']['elements']\n",
    "            print(f\"Found {len(elements)} elements\")\n",
    "            \n",
    "            # Extract titles from elements that have content.title\n",
    "            for element in elements:\n",
    "                try:\n",
    "                    if 'content' in element and 'title' in element['content']:\n",
    "                        titles.append({\n",
    "                            'title': element['content']['title'],\n",
    "                            'source': '20min.ch',\n",
    "                            'timestamp': datetime.now().isoformat()\n",
    "                        })\n",
    "                except:\n",
    "                    # Skip elements without proper structure\n",
    "                    continue\n",
    "        \n",
    "        print(f\"\u2705 Extracted {len(titles)} article titles from 20min.ch\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        twentymin_df = pd.DataFrame(titles)\n",
    "        \n",
    "        # Show sample\n",
    "        if not twentymin_df.empty:\n",
    "            print(\"\\n\ud83d\udcf0 Sample 20min.ch headlines:\")\n",
    "            print(\"=\" * 80)\n",
    "            for _, row in twentymin_df.head(10).iterrows():\n",
    "                print(f\"- {row['title']}\")\n",
    "        \n",
    "        return twentymin_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute the function\n",
    "twentymin_df = fetch_20min_titles()\n",
    "\n",
    "print(f\"\\n\u2728 DataFrame stored in variable 'twentymin_df' with {len(twentymin_df)} titles\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "bad33a84-373b-4095-92ef-48a1223ef8d1",
   "metadata": {},
   "outputs": [],
   "source": "import anthropic\nimport json\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Load environment variables from .env file\ntry:\n    from dotenv import load_dotenv\n    \n    if Path('.env').exists():\n        load_dotenv('.env')\n        print(\"\u2705 Loaded .env file from current directory\")\n    elif Path('../.env').exists():\n        load_dotenv('../.env')\n        print(\"\u2705 Loaded .env file from parent directory\")\n    else:\n        load_dotenv()\n        print(\"\u2705 Loaded .env file from default location\")\n        \nexcept ImportError:\n    print(\"\u274c python-dotenv not installed! Run: pip install python-dotenv\")\n    raise\n\n# Get API key from environment variable\nAPI_KEY = os.environ.get('ANTHROPIC_API_KEY')\n\n# Validate that we have the API key\nif not API_KEY:\n    print(\"\u274c ANTHROPIC_API_KEY not found in environment variables!\")\n    raise ValueError(\"ANTHROPIC_API_KEY environment variable not set!\")\nelse:\n    masked_key = f\"{API_KEY[:10]}...{API_KEY[-4:]}\" if len(API_KEY) > 20 else \"KEY_TOO_SHORT\"\n    print(f\"\u2705 API Key loaded successfully: {masked_key}\")\n\n# Initialize the Claude client\nclient = anthropic.Anthropic(api_key=API_KEY)\n\n# Load prompt from external file\ndef load_prompt_template(prompt_file='analysis_prompt.txt'):\n    \"\"\"Load the prompt template from external file.\"\"\"\n    try:\n        with open(prompt_file, 'r', encoding='utf-8') as f:\n            prompt_template = f.read()\n        print(f\"\u2705 Loaded prompt template from {prompt_file}\")\n        return prompt_template\n    except FileNotFoundError:\n        print(f\"\u274c Prompt file '{prompt_file}' not found!\")\n        raise\n    except Exception as e:\n        print(f\"\u274c Error loading prompt file: {str(e)}\")\n        raise\n\n# Create combined data structure for analysis\ndef prepare_data_for_analysis(competitor_df, twentymin_df):\n    \"\"\"Prepare combined data structure for Claude analysis.\"\"\"\n    \n    # Create combined data structure\n    combined_data = {\n        \"competitor_articles\": competitor_df.to_dict('records'),\n        \"twentymin_articles\": twentymin_df.to_dict('records'),\n        \"statistics\": {\n            \"total_competitor_articles\": len(competitor_df),\n            \"total_twentymin_articles\": len(twentymin_df),\n            \"competitor_sources\": competitor_df['source'].nunique() if 'source' in competitor_df.columns else 0,\n            \"timestamp\": datetime.now().isoformat()\n        }\n    }\n    \n    return combined_data\n\n# Function to make the API call\ndef analyze_competitor_gaps(competitor_df, twentymin_df):\n    \"\"\"\n    Send both DataFrames to Claude API for gap analysis\n    \"\"\"\n    # Prepare the data\n    combined_data = prepare_data_for_analysis(competitor_df, twentymin_df)\n    \n    # Convert to JSON\n    df_json = json.dumps(combined_data, ensure_ascii=False, indent=2)\n    \n    # Load and prepare prompt\n    prompt_template = load_prompt_template()\n    prompt = prompt_template.format(df_json=df_json)\n    \n    print(\"\\nSending request to Claude API...\")\n    print(f\"Analyzing {len(competitor_df)} competitor articles vs {len(twentymin_df)} 20min articles...\")\n    \n    try:\n        # Make the API call\n        response = client.messages.create(\n            model=\"claude-3-5-sonnet-20241022\",  # Using Sonnet 3.5\n            max_tokens=4000,  # Increased for comprehensive analysis\n            temperature=0.7,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ]\n        )\n        \n        # Extract the response\n        analysis = response.content[0].text\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"COMPETITOR GAP ANALYSIS FOR 20MIN.CH\")\n        print(\"=\"*80 + \"\\n\")\n        print(analysis)\n        \n        return analysis\n        \n    except Exception as e:\n        print(f\"\\n\u274c Error calling Claude API: {str(e)}\")\n        return None\n\n# Run the analysis\nprint(\"\ud83d\ude80 Starting competitor gap analysis for 20min.ch...\")\nprint(f\"\ud83d\udcca Competitor articles: {len(news_df)}\")\nprint(f\"\ud83d\udcca 20min articles: {len(twentymin_df)}\")\n\n# Call the function with both DataFrames\nanalysis_result = analyze_competitor_gaps(news_df, twentymin_df)\n\n# Store results\nif analysis_result:\n    print(\"\\n\u2705 Analysis complete! Check the output above for missing content opportunities.\")\n    \n    # Optional: Save to file\n    with open(f'competitor_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt', 'w', encoding='utf-8') as f:\n        f.write(analysis_result)\n    print(f\"\\n\ud83d\udcbe Analysis saved to competitor_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\")",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c75f64-47f3-4ef6-b09e-084d957203f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from jinja2 import Template\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"\u2705 Loaded .env file\")\n",
    "except ImportError:\n",
    "    print(\"\ud83d\udcdd dotenv not available - using system environment variables\")\n",
    "\n",
    "# Your Zapier webhook URL\n",
    "ZAPIER_WEBHOOK_URL = os.environ.get('ZAPIER_WEBHOOK_URL')\n",
    "\n",
    "# Add error checking\n",
    "if not ZAPIER_WEBHOOK_URL:\n",
    "    raise ValueError(\"ZAPIER_WEBHOOK_URL environment variable not set!\")\n",
    "\n",
    "def load_template(template_file='competitor_email_template.html'):\n",
    "    \"\"\"Load HTML template from file.\"\"\"\n",
    "    try:\n",
    "        with open(template_file, 'r', encoding='utf-8') as f:\n",
    "            template_content = f.read()\n",
    "        return Template(template_content)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\u274c Template file '{template_file}' not found!\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error loading template file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def extract_title_and_url(text):\n",
    "    \"\"\"Extract title and URL from Claude's formatted text.\"\"\"\n",
    "    url_pattern = r'(.*?)\\s*-\\s*(https?://[^\\s]+)'\n",
    "    match = re.search(url_pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        title = match.group(1).strip().strip('\"')\n",
    "        url = match.group(2).strip()\n",
    "        return {'text': title, 'url': url}\n",
    "    else:\n",
    "        clean_text = text.strip().strip('\"')\n",
    "        return {'text': clean_text, 'url': None}\n",
    "\n",
    "def parse_analysis_for_email(analysis_text):\n",
    "    \"\"\"Parse the Claude analysis text into structured data for email template.\"\"\"\n",
    "    \n",
    "    categories = []\n",
    "    keywords = []\n",
    "    recommendations = []\n",
    "    audience_recommendations = []\n",
    "    \n",
    "    lines = analysis_text.split('\\n')\n",
    "    current_section = None\n",
    "    current_category = None\n",
    "    current_audience = None\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Detect main category sections\n",
    "        if line.endswith(':') and any(word in line.upper() for word in ['POLITIK', 'WIRTSCHAFT', 'GESELLSCHAFT', 'LIFESTYLE', 'WISSENSCHAFT', 'TECHNOLOGIE', 'DIGITAL', 'BUSINESS', 'SPORT', 'HEALTH']):\n",
    "            category_name = line.replace(':', '').strip()\n",
    "            current_category = {\n",
    "                'name': category_name,\n",
    "                'stories': []\n",
    "            }\n",
    "            categories.append(current_category)\n",
    "            current_section = \"categories\"\n",
    "            continue\n",
    "        \n",
    "        # Detect numbered items within categories\n",
    "        elif current_section == \"categories\" and current_category and line and (line[0].isdigit() and '. ' in line):\n",
    "            story_data = extract_title_and_url(line)\n",
    "            current_category['stories'].append(story_data)\n",
    "            continue\n",
    "            \n",
    "        # Detect Keywords section\n",
    "        elif line == \"Keywords:\" or line == \"Schl\u00fcsselw\u00f6rter:\" or \"**Keywords:**\" in line:\n",
    "            current_section = \"keywords\"\n",
    "            continue\n",
    "        elif current_section == \"keywords\" and line.startswith('- '):\n",
    "            keyword = line[2:].strip()\n",
    "            keywords.append(keyword)\n",
    "            continue\n",
    "        \n",
    "        # Detect audience-specific recommendations\n",
    "        elif line.startswith('F\u00fcr ') and ('Gruppe' in line or 'Nutzer' in line) and line.endswith(':'):\n",
    "            current_section = \"audience\"\n",
    "            audience_name = line.replace(':', '').strip()\n",
    "            current_audience = {\n",
    "                'name': audience_name,\n",
    "                'items': []\n",
    "            }\n",
    "            audience_recommendations.append(current_audience)\n",
    "            continue\n",
    "        elif current_section == \"audience\" and current_audience and line.startswith('- '):\n",
    "            item_text = line[2:]\n",
    "            item_data = extract_title_and_url(item_text)\n",
    "            current_audience['items'].append(item_data)\n",
    "            continue\n",
    "        \n",
    "        # Detect Final recommendations section\n",
    "        elif \"Finale Top-Empfehlungen:\" in line:\n",
    "            current_section = \"recommendations\"\n",
    "            current_category = None\n",
    "            current_audience = None\n",
    "            continue\n",
    "        elif current_section == \"recommendations\" and line and (line[0].isdigit() and '. ' in line):\n",
    "            rec_data = extract_title_and_url(line)\n",
    "            recommendations.append(rec_data)\n",
    "            continue\n",
    "        \n",
    "        # Reset section if we hit a new major section\n",
    "        elif any(keyword in line for keyword in [\"**Zielgruppenspezifische Empfehlungen:**\", \"**Top-Themen nach Kategorien\"]):\n",
    "            current_section = None\n",
    "            current_category = None\n",
    "            current_audience = None\n",
    "    \n",
    "    return {\n",
    "        'categories': categories[:5],\n",
    "        'keywords': keywords[:10],\n",
    "        'recommendations': recommendations[:5],\n",
    "        'audience_recommendations': audience_recommendations\n",
    "    }\n",
    "\n",
    "def create_email_html(analysis_text, competitor_df, twentymin_df):\n",
    "    \"\"\"Create HTML content for email using competitor analysis template.\"\"\"\n",
    "    try:\n",
    "        template = load_template('competitor_email_template.html')\n",
    "        parsed_data = parse_analysis_for_email(analysis_text)\n",
    "        \n",
    "        print(f\"\ud83d\udcca Parsed {len(parsed_data['categories'])} categories\")\n",
    "        print(f\"\ud83d\udd11 Parsed {len(parsed_data['keywords'])} keywords\")\n",
    "        print(f\"\ud83d\udca1 Parsed {len(parsed_data['recommendations'])} recommendations\")\n",
    "        print(f\"\ud83c\udfaf Parsed {len(parsed_data['audience_recommendations'])} audience groups\")\n",
    "        \n",
    "        html_content = template.render(\n",
    "            date=datetime.now().strftime('%d.%m.%Y'),\n",
    "            time=datetime.now().strftime('%H:%M CET'),\n",
    "            stats={\n",
    "                'competitor_articles': len(competitor_df),\n",
    "                'twentymin_articles': len(twentymin_df),\n",
    "                'competitor_sources': competitor_df['source'].nunique() if 'source' in competitor_df.columns else 0\n",
    "            },\n",
    "            categories=parsed_data['categories'],\n",
    "            keywords=parsed_data['keywords'],\n",
    "            recommendations=parsed_data['recommendations'],\n",
    "            audience_recommendations=parsed_data['audience_recommendations']\n",
    "        )\n",
    "        \n",
    "        print(\"\u2705 Generated email content using template\")\n",
    "        return html_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error creating email HTML: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def send_to_zapier(analysis_text, competitor_df, twentymin_df):\n",
    "    \"\"\"Send the analysis via Zapier webhook for email delivery\"\"\"\n",
    "    print(\"\ud83d\udce4 Sending data to Zapier...\")\n",
    "    \n",
    "    payload = {\n",
    "        \"date\": datetime.now().strftime(\"%d.%m.%Y\"),\n",
    "        \"time\": datetime.now().strftime(\"%H:%M CET\"),\n",
    "        \"email_content_html\": create_email_html(analysis_text, competitor_df, twentymin_df),\n",
    "        \"stats\": {\n",
    "            \"competitor_articles\": len(competitor_df),\n",
    "            \"twentymin_articles\": len(twentymin_df),\n",
    "            \"competitor_sources\": competitor_df['source'].nunique() if 'source' in competitor_df.columns else 0\n",
    "        },\n",
    "        \"recipient_email\": \"tom.vaillant@20minuten.ch\",\n",
    "        \"email_subject\": f\"Competitor Gap Analysis - {datetime.now().strftime('%d.%m.%Y')}\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(ZAPIER_WEBHOOK_URL, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"\u2705 Successfully sent to Zapier!\")\n",
    "            print(f\"\ud83d\udce7 Email will be sent to: {payload['recipient_email']}\")\n",
    "            print(f\"\ud83d\udccb Subject: {payload['email_subject']}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\u274c Error sending to Zapier: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Exception occurred: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Main execution\n",
    "if 'analysis_result' in globals() and 'news_df' in globals() and 'twentymin_df' in globals():\n",
    "    success = send_to_zapier(analysis_result, news_df, twentymin_df)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\ud83c\udf89 All done! Check your email for the competitor gap analysis.\")\n",
    "else:\n",
    "    print(\"\u274c Missing required data. Please ensure all previous cells have been run:\")\n",
    "    print(\"   - news_df (competitor articles)\")\n",
    "    print(\"   - twentymin_df (20min articles)\")\n",
    "    print(\"   - analysis_result (Claude's analysis)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}