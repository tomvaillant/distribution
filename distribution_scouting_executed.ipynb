{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3356e4b0-9c0e-48c6-84d2-4f1761b115b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T00:21:52.270246Z",
     "iopub.status.busy": "2025-06-07T00:21:52.269727Z",
     "iopub.status.idle": "2025-06-07T00:22:24.230730Z",
     "shell.execute_reply": "2025-06-07T00:22:24.230022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting competitor scraper...\n",
      "‚úÖ Loaded 13 competitor sources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Collected 130 articles from 13 sources\n",
      "‚ú® DataFrame 'news_df' ready with 130 articles\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "# Load competitor sources from CSV file\n",
    "def load_competitor_sources_from_csv(csv_path='competitor_sources.csv'):\n",
    "    \"\"\"Load competitor sources from CSV file.\"\"\"\n",
    "    try:\n",
    "        sources_df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úÖ Loaded {len(sources_df)} competitor sources\")\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        news_feeds = []\n",
    "        for _, row in sources_df.iterrows():\n",
    "            news_feeds.append({\n",
    "                'name': row['name'],\n",
    "                'url': row['url']\n",
    "            })\n",
    "        \n",
    "        return news_feeds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CSV: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Function to extract time ago from span text\n",
    "def extract_time_ago(time_text):\n",
    "    \"\"\"Convert time ago text to minutes for sorting.\"\"\"\n",
    "    if not time_text:\n",
    "        return 999999\n",
    "    \n",
    "    time_text = time_text.lower()\n",
    "    if 'm' in time_text or 'min' in time_text:\n",
    "        match = re.search(r'(\\d+)', time_text)\n",
    "        return int(match.group(1)) if match else 999999\n",
    "    elif 'h' in time_text or 'hour' in time_text:\n",
    "        match = re.search(r'(\\d+)', time_text)\n",
    "        return int(match.group(1)) * 60 if match else 999999\n",
    "    elif 'd' in time_text or 'day' in time_text:\n",
    "        match = re.search(r'(\\d+)', time_text)\n",
    "        return int(match.group(1)) * 1440 if match else 999999\n",
    "    else:\n",
    "        return 999999\n",
    "\n",
    "# Function to scrape Bing News results\n",
    "def scrape_bing_news(url, source_name, max_articles=10):\n",
    "    \"\"\"Scrape news articles from Bing News search results.\"\"\"\n",
    "    articles = []\n",
    "    \n",
    "    try:\n",
    "        # Headers to mimic a real browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "        \n",
    "        # Make request\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all news cards\n",
    "        news_cards = soup.find_all('div', class_='news-card')\n",
    "        \n",
    "        if not news_cards:\n",
    "            news_cards = soup.find_all('div', {'class': re.compile('newsitem|news-card-body')})\n",
    "        \n",
    "        for i, card in enumerate(news_cards[:max_articles]):\n",
    "            try:\n",
    "                article_data = {}\n",
    "                \n",
    "                # Extract title\n",
    "                title_elem = card.find('a', class_='title')\n",
    "                if not title_elem:\n",
    "                    title_elem = card.find('a', {'class': re.compile('title|headline')})\n",
    "                \n",
    "                if title_elem:\n",
    "                    article_data['title'] = title_elem.get_text(strip=True)\n",
    "                    article_data['link'] = title_elem.get('href', '')\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Extract snippet/description\n",
    "                snippet_elem = card.find('div', class_='snippet')\n",
    "                if not snippet_elem:\n",
    "                    snippet_elem = card.find('div', {'class': re.compile('snippet|summary|description')})\n",
    "                \n",
    "                article_data['description'] = snippet_elem.get_text(strip=True) if snippet_elem else ''\n",
    "                \n",
    "                # Extract time\n",
    "                time_elem = card.find('span', {'aria-label': re.compile('ago|minutes|hours|days')})\n",
    "                if not time_elem:\n",
    "                    time_elem = card.find('span', text=re.compile(r'\\d+[mhd]'))\n",
    "                \n",
    "                time_text = time_elem.get_text(strip=True) if time_elem else 'Unknown'\n",
    "                article_data['time_ago'] = time_text\n",
    "                article_data['minutes_ago'] = extract_time_ago(time_text)\n",
    "                \n",
    "                # Extract source\n",
    "                source_elem = card.find('a', {'aria-label': re.compile('Search news from')})\n",
    "                if source_elem:\n",
    "                    article_data['original_source'] = source_elem.get_text(strip=True)\n",
    "                else:\n",
    "                    article_data['original_source'] = source_name\n",
    "                \n",
    "                article_data['source'] = source_name\n",
    "                article_data['timestamp'] = datetime.now().isoformat()\n",
    "                \n",
    "                articles.append(article_data)\n",
    "                \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Sort by recency\n",
    "        articles.sort(key=lambda x: x['minutes_ago'])\n",
    "        \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Main execution\n",
    "print(\"üöÄ Starting competitor scraper...\")\n",
    "\n",
    "# Load competitor sources\n",
    "NEWS_FEEDS = load_competitor_sources_from_csv()\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Process each feed\n",
    "for feed in NEWS_FEEDS:\n",
    "    articles = scrape_bing_news(feed['url'], feed['name'])\n",
    "    \n",
    "    # Add each article to results\n",
    "    for article in articles:\n",
    "        article['feed_url'] = feed['url']\n",
    "        all_results.append(article)\n",
    "    \n",
    "    # Delay to be respectful\n",
    "    time.sleep(2)\n",
    "\n",
    "# Create DataFrame\n",
    "news_df = pd.DataFrame(all_results)\n",
    "\n",
    "if not news_df.empty:\n",
    "    print(f\"‚úÖ Collected {len(news_df)} articles from {news_df['source'].nunique()} sources\")\n",
    "else:\n",
    "    print(\"‚ùå No articles collected!\")\n",
    "\n",
    "print(f\"‚ú® DataFrame 'news_df' ready with {len(news_df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f76284-ced9-49c8-a914-68b92df8d33c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T00:22:24.232965Z",
     "iopub.status.busy": "2025-06-07T00:22:24.232568Z",
     "iopub.status.idle": "2025-06-07T00:22:24.761442Z",
     "shell.execute_reply": "2025-06-07T00:22:24.760815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ Fetching 20min.ch articles...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 149 article titles\n",
      "‚ú® DataFrame 'twentymin_df' ready with 149 titles\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_20min_titles():\n",
    "    \"\"\"Fetch titles from 20min.ch API.\"\"\"\n",
    "    \n",
    "    url = \"https://api.20min.ch/content/6/category/1\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'de-CH,de;q=0.9,en;q=0.8',\n",
    "        'Origin': 'https://www.20min.ch',\n",
    "        'Referer': 'https://www.20min.ch/',\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"üì∞ Fetching 20min.ch articles...\")\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        titles = []\n",
    "        \n",
    "        if 'content' in data and 'elements' in data['content']:\n",
    "            elements = data['content']['elements']\n",
    "            \n",
    "            for element in elements:\n",
    "                try:\n",
    "                    if 'content' in element and 'title' in element['content']:\n",
    "                        titles.append({\n",
    "                            'title': element['content']['title'],\n",
    "                            'source': '20min.ch',\n",
    "                            'timestamp': datetime.now().isoformat()\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        twentymin_df = pd.DataFrame(titles)\n",
    "        print(f\"‚úÖ Extracted {len(titles)} article titles\")\n",
    "        \n",
    "        return twentymin_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Execute the function\n",
    "twentymin_df = fetch_20min_titles()\n",
    "print(f\"‚ú® DataFrame 'twentymin_df' ready with {len(twentymin_df)} titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad33a84-373b-4095-92ef-48a1223ef8d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T00:22:24.763670Z",
     "iopub.status.busy": "2025-06-07T00:22:24.763276Z",
     "iopub.status.idle": "2025-06-07T00:22:54.348179Z",
     "shell.execute_reply": "2025-06-07T00:22:54.347461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded\n",
      "üöÄ Starting competitor analysis...\n",
      "ü§ñ Analyzing 130 vs 149 articles...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis complete\n",
      "‚ú® Analysis stored in 'analysis_result' variable\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Environment loaded\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå python-dotenv not installed\")\n",
    "    raise\n",
    "\n",
    "# Get API key\n",
    "API_KEY = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY environment variable not set!\")\n",
    "\n",
    "# Initialize Claude client\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "def load_prompt_template(prompt_file='analysis_prompt.txt'):\n",
    "    \"\"\"Load the prompt template from external file.\"\"\"\n",
    "    try:\n",
    "        with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "            prompt_template = f.read()\n",
    "        return prompt_template\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading prompt file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def prepare_data_for_analysis(competitor_df, twentymin_df):\n",
    "    \"\"\"Prepare combined data structure for Claude analysis.\"\"\"\n",
    "    \n",
    "    combined_data = {\n",
    "        \"competitor_articles\": competitor_df.to_dict('records'),\n",
    "        \"twentymin_articles\": twentymin_df.to_dict('records'),\n",
    "        \"statistics\": {\n",
    "            \"total_competitor_articles\": len(competitor_df),\n",
    "            \"total_twentymin_articles\": len(twentymin_df),\n",
    "            \"competitor_sources\": competitor_df['source'].nunique() if 'source' in competitor_df.columns else 0,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "def analyze_competitor_gaps(competitor_df, twentymin_df):\n",
    "    \"\"\"Send both DataFrames to Claude API for gap analysis\"\"\"\n",
    "    \n",
    "    combined_data = prepare_data_for_analysis(competitor_df, twentymin_df)\n",
    "    df_json = json.dumps(combined_data, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    prompt_template = load_prompt_template()\n",
    "    prompt = prompt_template.format(df_json=df_json)\n",
    "    \n",
    "    print(f\"ü§ñ Analyzing {len(competitor_df)} vs {len(twentymin_df)} articles...\")\n",
    "    \n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=4000,\n",
    "            temperature=0.7,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        analysis = response.content[0].text\n",
    "        print(\"‚úÖ Analysis complete\")\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calling Claude API: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "print(\"üöÄ Starting competitor analysis...\")\n",
    "analysis_result = analyze_competitor_gaps(news_df, twentymin_df)\n",
    "\n",
    "if analysis_result:\n",
    "    print(\"‚ú® Analysis stored in 'analysis_result' variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c75f64-47f3-4ef6-b09e-084d957203f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T00:22:54.350545Z",
     "iopub.status.busy": "2025-06-07T00:22:54.350149Z",
     "iopub.status.idle": "2025-06-07T00:22:54.491004Z",
     "shell.execute_reply": "2025-06-07T00:22:54.490336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded\n",
      "üì§ Sending to Zapier...\n",
      "‚úÖ Email sent successfully!\n",
      "üéâ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from jinja2 import Template\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Environment loaded\")\n",
    "except ImportError:\n",
    "    print(\"üìù Using system environment variables\")\n",
    "\n",
    "ZAPIER_WEBHOOK_URL = os.environ.get('ZAPIER_WEBHOOK_URL')\n",
    "\n",
    "if not ZAPIER_WEBHOOK_URL:\n",
    "    raise ValueError(\"ZAPIER_WEBHOOK_URL environment variable not set!\")\n",
    "\n",
    "def load_template(template_file='competitor_email_template.html'):\n",
    "    \"\"\"Load HTML template from file.\"\"\"\n",
    "    try:\n",
    "        with open(template_file, 'r', encoding='utf-8') as f:\n",
    "            template_content = f.read()\n",
    "        return Template(template_content)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading template: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def extract_title_and_url(text):\n",
    "    \"\"\"Extract title and URL from Claude's formatted text.\"\"\"\n",
    "    text = text.strip().lstrip('- ').strip('\"')\n",
    "    \n",
    "    url_pattern = r'(.*?)\\s*-\\s*(https?://[^\\s]+)'\n",
    "    match = re.search(url_pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        title = match.group(1).strip().strip('\"')\n",
    "        url = match.group(2).strip()\n",
    "        return {'text': title, 'url': url}\n",
    "    else:\n",
    "        clean_text = text.strip().strip('\"')\n",
    "        return {'text': clean_text, 'url': None}\n",
    "\n",
    "def parse_analysis_for_email(analysis_text):\n",
    "    \"\"\"Parse the Claude analysis text into structured data for email template.\"\"\"\n",
    "    \n",
    "    categories = []\n",
    "    keywords = []\n",
    "    recommendations = []\n",
    "    audience_recommendations = []\n",
    "    \n",
    "    lines = analysis_text.split('\\n')\n",
    "    current_section = None\n",
    "    current_category = None\n",
    "    current_audience = None\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Detect main category sections\n",
    "        if line.endswith(':') and any(word in line.upper() for word in ['POLITIK', 'WIRTSCHAFT', 'GESELLSCHAFT', 'LIFESTYLE', 'WISSENSCHAFT', 'TECHNOLOGIE', 'DIGITAL', 'BUSINESS', 'SPORT', 'HEALTH']):\n",
    "            category_name = line.replace(':', '').strip()\n",
    "            current_category = {\n",
    "                'name': category_name,\n",
    "                'stories': []\n",
    "            }\n",
    "            categories.append(current_category)\n",
    "            current_section = \"categories\"\n",
    "            continue\n",
    "        \n",
    "        # Detect numbered items within categories\n",
    "        elif current_section == \"categories\" and current_category and line and (line[0].isdigit() and '. ' in line):\n",
    "            story_data = extract_title_and_url(line)\n",
    "            current_category['stories'].append(story_data)\n",
    "            continue\n",
    "            \n",
    "        # Detect Keywords section\n",
    "        elif line == \"Keywords:\" or line == \"Schl√ºsselw√∂rter:\" or \"**Keywords:**\" in line:\n",
    "            current_section = \"keywords\"\n",
    "            continue\n",
    "        elif current_section == \"keywords\" and line.startswith('- '):\n",
    "            keyword = line[2:].strip()\n",
    "            keywords.append(keyword)\n",
    "            continue\n",
    "        \n",
    "        # Detect TOP 5 STORY-EMPFEHLUNGEN section\n",
    "        elif \"TOP 5 STORY-EMPFEHLUNGEN:\" in line or \"Finale Top-Empfehlungen:\" in line:\n",
    "            current_section = \"recommendations\"\n",
    "            current_category = None\n",
    "            current_audience = None\n",
    "            continue\n",
    "        elif current_section == \"recommendations\" and line and (line[0].isdigit() and '. ' in line):\n",
    "            rec_data = extract_title_and_url(line)\n",
    "            recommendations.append(rec_data)\n",
    "            continue\n",
    "        \n",
    "        # Detect audience-specific recommendations with \"F√ºr\" pattern\n",
    "        elif line.startswith('F√ºr ') and line.endswith(':'):\n",
    "            current_section = \"audience\"\n",
    "            audience_name = line.replace(':', '').strip()\n",
    "            current_audience = {\n",
    "                'name': audience_name,\n",
    "                'items': []\n",
    "            }\n",
    "            audience_recommendations.append(current_audience)\n",
    "            continue\n",
    "        elif current_section == \"audience\" and current_audience and line.startswith('- '):\n",
    "            item_text = line[2:]\n",
    "            item_data = extract_title_and_url(item_text)\n",
    "            current_audience['items'].append(item_data)\n",
    "            continue\n",
    "        \n",
    "        # Reset section if we hit a new major section\n",
    "        elif any(keyword in line for keyword in [\"**Zielgruppenspezifische Empfehlungen:**\", \"**Top-Themen nach Kategorien\"]):\n",
    "            current_section = None\n",
    "            current_category = None\n",
    "            current_audience = None\n",
    "    \n",
    "    return {\n",
    "        'categories': categories[:5],\n",
    "        'keywords': keywords[:10],\n",
    "        'recommendations': recommendations[:5],\n",
    "        'audience_recommendations': audience_recommendations\n",
    "    }\n",
    "\n",
    "def create_email_html(analysis_text, competitor_df, twentymin_df):\n",
    "    \"\"\"Create HTML content for email using competitor analysis template.\"\"\"\n",
    "    try:\n",
    "        template = load_template('competitor_email_template.html')\n",
    "        parsed_data = parse_analysis_for_email(analysis_text)\n",
    "        \n",
    "        html_content = template.render(\n",
    "            date=datetime.now().strftime('%d.%m.%Y'),\n",
    "            time=datetime.now().strftime('%H:%M CET'),\n",
    "            stats={\n",
    "                'competitor_articles': len(competitor_df),\n",
    "                'twentymin_articles': len(twentymin_df),\n",
    "                'competitor_sources': competitor_df['source'].nunique() if 'source' in competitor_df.columns else 0\n",
    "            },\n",
    "            categories=parsed_data['categories'],\n",
    "            keywords=parsed_data['keywords'],\n",
    "            recommendations=parsed_data['recommendations'],\n",
    "            audience_recommendations=parsed_data['audience_recommendations']\n",
    "        )\n",
    "        \n",
    "        return html_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating email HTML: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def send_to_zapier(analysis_text, competitor_df, twentymin_df):\n",
    "    \"\"\"Send the analysis via Zapier webhook for email delivery\"\"\"\n",
    "    print(\"üì§ Sending to Zapier...\")\n",
    "    \n",
    "    payload = {\n",
    "        \"date\": datetime.now().strftime(\"%d.%m.%Y\"),\n",
    "        \"time\": datetime.now().strftime(\"%H:%M CET\"),\n",
    "        \"email_content_html\": create_email_html(analysis_text, competitor_df, twentymin_df),\n",
    "        \"stats\": {\n",
    "            \"competitor_articles\": len(competitor_df),\n",
    "            \"twentymin_articles\": len(twentymin_df),\n",
    "            \"competitor_sources\": competitor_df['source'].nunique() if 'source' in competitor_df.columns else 0\n",
    "        },\n",
    "        \"recipient_email\": \"tom.vaillant@20minuten.ch\",\n",
    "        \"email_subject\": f\"Konkurrenz-L√ºckenanalyse - {datetime.now().strftime('%d.%m.%Y %H:%M CET')}\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(ZAPIER_WEBHOOK_URL, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Email sent successfully!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Main execution\n",
    "if 'analysis_result' in globals() and 'news_df' in globals() and 'twentymin_df' in globals():\n",
    "    success = send_to_zapier(analysis_result, news_df, twentymin_df)\n",
    "    \n",
    "    if success:\n",
    "        print(\"üéâ Analysis complete!\")\n",
    "else:\n",
    "    print(\"‚ùå Missing required data from previous cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e1820a-b7a2-457a-a826-059da58b9982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
